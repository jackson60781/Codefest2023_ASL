Inspiration:
When interacting with today's world, it has developed technologically advanced in conjunction with the use of artificial intelligence. As the world becomes increasingly reliant on technology and online communication, it's important to ensure that everyone has the ability to communicate effectively. However, for people who rely on American Sign Language as their primary mode of communication, this can present a challenge. Our project aims to address this issue by developing a system that uses gesture recognition technology to interpret American Sign Language and translate it into text in real-time. By doing so, we aim to make online communication more accessible and inclusive for people who use sign language. Our project leverages advanced technologies such as MediaPipe, TensorFlow, and OpenCV to achieve this goal. Our system will enable people who use sign language to communicate more effectively in online settings, opening up new opportunities and enhancing their ability to participate in today's increasingly digital world

What it does:
GestureBridge: ASL Translate interprets hand motion gestures using a camera sensor to be able to convert and display the corresponding alphabet of the American Sign Language in order to be able to communicate more efficiently with individuals who prefer this mode of communication and make it more accessible to them.

How we built it:
Our team utilized OpenCV, Tensorflow, and Mediapipe libraries in order to build our AI model. OpenCV is to recognize the hand gestures, Tensorflow to train the models using our own dataset, and Mediapipe to model the hand. We also used cvzone and teachable machine with google.

Challenges we ran into:
One challenge that our team ran into is being able to train the large dataset for our American Sign Language AI program, where it would take long periods of time. To overcome this challenge, our consensus is to generate our own dataset where we can provide as the optimal dataset amount while also being able to generate a high enough accuracy. In addition to that, there were some hardware limitations that we have occurred which delayed our development.

Accomplishments that we're proud of:
As none of us have utilized AI libraries or have any actual experience coding in AI, it made it even more challenging. However, we were able to successfully implement and train our dataset to be able to recognize gestures from the American Sign Language Alphabet.

What we learned:
We learned how to utilize the Mediapipe, and OpenCV in order to model our hand and then build our dataset in order to be able to train the AI with Tensorflow.

What's next for GestureBridge: ASL Translate
There is endless potential for what we can do with our program. One of our next goals is for our program to be able to educate and have real-time feedback regarding learning the American Sign Language gestures for individual alphabets and numbers.

Built With
cvzone
mediapipe
opencv
python
teachablemachine
tensorflow
